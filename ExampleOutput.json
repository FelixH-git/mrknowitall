{"tag": "0", "url": "https://proceedings.neurips.cc/paper/2021/hash/854d9fca60b4bd07f9bb215d59ef5561-Abstract.html", "content": "The paper \"Transformer in Transformer\" investigates a new type of neural architecture that encodes input data using the attention mechanism. This approach is used for visual transformers, which divide images into several local patches, calculations representations and relationships. Since natural images have highly complex details, the paper suggests that the granularity of patch division isn't sufficient to excavate object features. The paper introduces a new architecture, Transformer in Transformer (TNT), that treats local patches as \"visual sentences\" and divides them into smaller patches or \"visual words\". This technique improves the transformers' performance. The new architecture showed better accuracy in ImageNet benchmark tests, improving by 1.7% compared to state-of-the-art visual transformers of similar computational cost. The PyTorch code is publicly available."}